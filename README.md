# ğŸ¦™ LLaMA Text Summarizer

A simple text summarization app using the **LLaMA model via Ollama**, powered by **FastAPI** (backend) and **Streamlit** (frontend). Run everything locally and summarize large chunks of text in seconds.

---

## ğŸ” Overview

- ğŸ” Summarize long text using LLaMA locally  
- âš¡ FastAPI backend to interact with the model  
- ğŸ¨ Streamlit frontend for user interaction  
- ğŸ§ª LLM runs locally using [Ollama](https://ollama.com)

---

## ğŸ› ï¸ Tech Stack

- ğŸ¦™ **LLaMA** (via Ollama)  
- ğŸš€ **FastAPI** â€“ RESTful backend  
- ğŸ–¥ **Streamlit** â€“ Interactive frontend UI  
- ğŸ—‚ **Git/GitHub** â€“ Version control & collaboration


---
# ğŸ“ Project Structure
```
text-summarizer-llama/
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```

---

## âš™ï¸ Setup Instructions

### Step 1: Environment Setup

```
$ python -m venv venv

# On Windows
$ venv\Scripts\activate

# On macOS/Linux
$ source venv/bin/activate
```

#### Install dependencies:

```
$ pip install -r requirements.txt
```

### Step 2: Install & Run Ollama

Download Ollama and install it.

#### Start Ollama and pull the LLaMA model:

```
$ ollama pull llama2
```

#### Run the App
Start the backend using FastAPI:

```
$ uvicorn backend.main:app --reload
```

Start the frontend using Streamlit:

```
$ streamlit run frontend/app.py
```

#### Then open the app in your browser, enter any text, and get a clean summary powered by LLaMA.


---
## ğŸ“ƒ License

This project is licensed under the MIT License.
Feel free to use, modify, and share with attribution.

---

## ğŸ¤ Contributing

Pull requests are welcome. For major changes, please open an issue first to discuss what youâ€™d like to change.

---

## ğŸŒ Author
#### Hardik Sankhla

